{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Engineering Pipeline: Social Media Metrics\n",
        "\n",
        "This notebook explains the **end-to-end data engineering pipeline** for turning raw unstructured PDF reports into queryable analytical data for business analysts.\n",
        "\n",
        "---\n",
        "\n",
        "## Pipeline Overview\n",
        "\n",
        "```\n",
        "SOURCE  →  INGESTION  →  RAW STORAGE  →  TRANSFORMATION  →  MODELLING  →  SERVING\n",
        "(PDFs)      (Extract)     (Postgres)      (Clean/Shape)     (Analytical)   (Analysts)\n",
        "```\n",
        "\n",
        "| Stage | Purpose |\n",
        "|-------|---------|\n",
        "| **Source** | Raw PDF files containing social media metrics (unstructured) |\n",
        "| **Ingestion** | Extract text/metrics from PDFs and load into the system |\n",
        "| **Raw Storage** | Persist raw extracted data in Postgres (bronze layer) |\n",
        "| **Transformation** | Clean, validate, and reshape data for analytics |\n",
        "| **Modelling** | Build analyst-friendly dimensional/fact models |\n",
        "| **Serving** | Serve data to end users via Postgres queries / BI tools |"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec741e75",
      "metadata": {},
      "source": [
        "### Setup & Dependencies\n",
        "\n",
        "Run the cell below once to ensure required packages are installed. The notebook uses:\n",
        "- `sqlalchemy` — database connectivity (Postgres or SQLite)\n",
        "- `pandas` — data transformation\n",
        "- `pdfplumber` — optional, for real PDF extraction (falls back to sample data if missing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a44f92a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# OPTIONAL: Install dependencies (uncomment and run once if needed)\n",
        "# !pip install sqlalchemy pandas pdfplumber\n",
        "\n",
        "# Verify core imports\n",
        "import sqlalchemy\n",
        "import pandas as pd\n",
        "print(\"Setup OK: sqlalchemy, pandas ready\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Stage 1: SOURCE — Raw Unstructured PDFs\n",
        "\n",
        "**What it is:** The origin of our data — PDF reports (often monthly/quarterly) containing social media metrics such as:\n",
        "- Impressions, reach, engagement (likes, comments, shares)\n",
        "- Follower growth, click-through rates\n",
        "- Platform-specific KPIs (Facebook, Instagram, LinkedIn, etc.)\n",
        "\n",
        "**Why it's challenging:** PDFs are unstructured. Text, tables, and charts are embedded in a binary format. We cannot query them directly — we must extract and structure the content first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# STAGE 1: SOURCE — Representing raw PDF files\n",
        "# =============================================================================\n",
        "# In practice, these PDFs live in a folder, S3 bucket, or shared drive.\n",
        "# We simulate the source by listing files we intend to ingest.\n",
        "# =============================================================================\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "# Example: directory containing monthly social media PDF reports\n",
        "SOURCE_DIR = Path(\"data/raw_reports\")  # e.g. ./data/raw_reports/2024-01_report.pdf\n",
        "\n",
        "# Each PDF file is a SOURCE document — unstructured, not queryable yet\n",
        "def list_source_pdfs(directory: Path) -> list[Path]:\n",
        "    \"\"\"\n",
        "    List all PDF files in the source directory.\n",
        "    These represent the raw, unstructured input to our pipeline.\n",
        "    \"\"\"\n",
        "    if not directory.exists():\n",
        "        return []\n",
        "    return sorted(directory.glob(\"*.pdf\"))\n",
        "\n",
        "# In production: SOURCE = S3 paths, Azure Blob, or file share\n",
        "source_files = list_source_pdfs(SOURCE_DIR)\n",
        "print(f\"Source: {len(source_files)} PDF file(s) identified for ingestion\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Stage 2: INGESTION — Extract and Load\n",
        "\n",
        "**What it is:** The process of reading PDFs, extracting text and tabular data, and preparing it for storage.\n",
        "\n",
        "**Key tasks:**\n",
        "- Open each PDF\n",
        "- Extract text, tables, and metadata (e.g. report date, platform name)\n",
        "- Normalise into a consistent structure (e.g. rows with platform, metric_name, value, report_date)\n",
        "- Load into Raw Storage\n",
        "\n",
        "**Tools:** Libraries like `PyMuPDF` (fitz), `pdfplumber`, or `tabula-py` for table extraction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# STAGE 2: INGESTION — Extract content from PDFs\n",
        "# =============================================================================\n",
        "# We extract text/tables and convert to a structured format (records)\n",
        "# that can be written to Postgres. This is the \"bronze\" or \"raw\" ingest.\n",
        "# =============================================================================\n",
        "\n",
        "# Optional: pip install pymupdf pdfplumber  (use one for PDF parsing)\n",
        "\n",
        "import re\n",
        "from datetime import datetime\n",
        "from typing import Any\n",
        "\n",
        "\n",
        "def extract_metrics_from_pdf(pdf_path: Path) -> list[dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    INGESTION: Parse a PDF and extract social media metrics as structured records.\n",
        "    \n",
        "    In a real implementation:\n",
        "    - Use pdfplumber or PyMuPDF to extract tables\n",
        "    - Map table columns to: platform, metric_name, value, report_date\n",
        "    - Return a list of dicts, one per metric row\n",
        "    \"\"\"\n",
        "    records = []\n",
        "    try:\n",
        "        import pdfplumber\n",
        "        with pdfplumber.open(pdf_path) as pdf:\n",
        "            for page in pdf.pages:\n",
        "                tables = page.extract_tables()\n",
        "                for table in tables or []:\n",
        "                    # Assume header row, then data rows\n",
        "                    # Example: [['Platform','Metric','Value'], ['Instagram','Impressions','15000'], ...]\n",
        "                    if len(table) < 2:\n",
        "                        continue\n",
        "                    headers = [str(h or \"\").strip().lower() for h in table[0]]\n",
        "                    for row in table[1:]:\n",
        "                        if not row:\n",
        "                            continue\n",
        "                        record = dict(zip(headers, [str(c or \"\").strip() for c in row]))\n",
        "                        # Add metadata from filename (e.g. 2024-01_report.pdf -> report_date)\n",
        "                        record[\"source_file\"] = pdf_path.name\n",
        "                        record[\"ingested_at\"] = datetime.utcnow().isoformat()\n",
        "                        records.append(record)\n",
        "    except ImportError:\n",
        "        # Fallback: simulate extracted data for demonstration\n",
        "        records = [\n",
        "            {\"platform\": \"Instagram\", \"metric_name\": \"impressions\", \"value\": \"15000\", \"source_file\": pdf_path.name, \"ingested_at\": datetime.utcnow().isoformat()},\n",
        "            {\"platform\": \"Instagram\", \"metric_name\": \"engagement\", \"value\": \"1200\", \"source_file\": pdf_path.name, \"ingested_at\": datetime.utcnow().isoformat()},\n",
        "            {\"platform\": \"LinkedIn\", \"metric_name\": \"impressions\", \"value\": \"8000\", \"source_file\": pdf_path.name, \"ingested_at\": datetime.utcnow().isoformat()},\n",
        "        ]\n",
        "    return records\n",
        "\n",
        "\n",
        "# Example: for each source PDF, extract and collect records for loading\n",
        "all_raw_records = []\n",
        "for pdf_file in source_files:\n",
        "    all_raw_records.extend(extract_metrics_from_pdf(pdf_file))\n",
        "\n",
        "# If no PDFs exist, create sample records for demo purposes\n",
        "if not all_raw_records:\n",
        "    all_raw_records = extract_metrics_from_pdf(Path(\"sample_report.pdf\"))\n",
        "\n",
        "print(f\"Ingestion complete: {len(all_raw_records)} raw records extracted\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Stage 3: RAW STORAGE — Postgres (Bronze Layer)\n",
        "\n",
        "**What it is:** A durable storage layer that holds the **raw ingested data** exactly as extracted — no cleansing or aggregation yet.\n",
        "\n",
        "**Why Postgres?**\n",
        "- ACID compliance, reliable storage\n",
        "- SQL query capability for downstream stages\n",
        "- Well-supported by BI tools and analysts\n",
        "\n",
        "**Schema design:** A simple table that stores each extracted record with metadata (source file, ingestion timestamp). This is the \"bronze\" or raw layer — we keep it as-is for replay and debugging."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# STAGE 3: RAW STORAGE — Persist ingested data in Postgres\n",
        "# =============================================================================\n",
        "# We create a table for the raw/bronze layer and insert ingested records.\n",
        "# Connection: set POSTGRES_URL or use sqlite for local demo.\n",
        "# =============================================================================\n",
        "\n",
        "import os\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "# Use Postgres if available; otherwise SQLite for local demo\n",
        "POSTGRES_URL = os.getenv(\"POSTGRES_URL\", \"postgresql://localhost:5432/social_metrics\")\n",
        "USE_SQLITE = os.getenv(\"USE_SQLITE\", \"true\").lower() == \"true\"  # Default: SQLite for portability\n",
        "\n",
        "if USE_SQLITE:\n",
        "    DB_URL = \"sqlite:///data/social_metrics_raw.db\"\n",
        "else:\n",
        "    DB_URL = POSTGRES_URL\n",
        "\n",
        "\n",
        "def create_raw_storage_schema(engine):\n",
        "    \"\"\"\n",
        "    RAW STORAGE schema: One table to store all raw ingested records.\n",
        "    We preserve original structure (JSONB in Postgres) for flexibility.\n",
        "    \"\"\"\n",
        "    from sqlalchemy import create_engine, text\n",
        "    \n",
        "    with engine.connect() as conn:\n",
        "        conn.execute(text(\"\"\"\n",
        "            CREATE TABLE IF NOT EXISTS raw_social_metrics (\n",
        "                id SERIAL PRIMARY KEY,\n",
        "                source_file VARCHAR(255),\n",
        "                ingested_at TIMESTAMP,\n",
        "                raw_json JSONB\n",
        "            )\n",
        "        \"\"\")) if \"postgresql\" in str(engine.url) else conn.execute(text(\"\"\"\n",
        "            CREATE TABLE IF NOT EXISTS raw_social_metrics (\n",
        "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                source_file VARCHAR(255),\n",
        "                ingested_at TIMESTAMP,\n",
        "                raw_json TEXT\n",
        "            )\n",
        "        \"\"\"))\n",
        "        conn.commit()\n",
        "\n",
        "\n",
        "def load_into_raw_storage(engine, records: list[dict]) -> int:\n",
        "    \"\"\"\n",
        "    INGESTION → RAW STORAGE: Insert each extracted record into Postgres.\n",
        "    \"\"\"\n",
        "    from sqlalchemy import create_engine, text\n",
        "    \n",
        "    count = 0\n",
        "    with engine.connect() as conn:\n",
        "        for r in records:\n",
        "            source_file = r.get(\"source_file\", \"unknown\")\n",
        "            ingested_at = r.get(\"ingested_at\", datetime.utcnow().isoformat())\n",
        "            raw_json = json.dumps({k: v for k, v in r.items() if k not in (\"source_file\", \"ingested_at\")})\n",
        "            conn.execute(text(\"\"\"\n",
        "                INSERT INTO raw_social_metrics (source_file, ingested_at, raw_json)\n",
        "                VALUES (:sf, :ia, :rj)\n",
        "            \"\"\"), {\"sf\": source_file, \"ia\": ingested_at, \"rj\": raw_json})\n",
        "            count += 1\n",
        "        conn.commit()\n",
        "    return count\n",
        "\n",
        "# Create engine and load data (simplified — in production use connection pooling)\n",
        "from sqlalchemy import create_engine\n",
        "Path(\"data\").mkdir(exist_ok=True)\n",
        "engine = create_engine(DB_URL)\n",
        "# For SQLite, schema differs slightly — we use a generic approach below\n",
        "print(f\"Raw storage: {DB_URL}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61aeb052",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Apply RAW STORAGE schema (SQLite-compatible version)\n",
        "# =============================================================================\n",
        "\n",
        "from sqlalchemy import text\n",
        "\n",
        "with engine.connect() as conn:\n",
        "    conn.execute(text(\"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS raw_social_metrics (\n",
        "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "            source_file VARCHAR(255),\n",
        "            ingested_at TIMESTAMP,\n",
        "            raw_json TEXT\n",
        "        )\n",
        "    \"\"\"))\n",
        "    conn.commit()\n",
        "\n",
        "loaded = load_into_raw_storage(engine, all_raw_records)\n",
        "print(f\"Loaded {loaded} records into raw storage\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Stage 4: TRANSFORMATION — Clean and Reshape\n",
        "\n",
        "**What it is:** Apply business rules to clean and standardise the raw data.\n",
        "\n",
        "**Typical steps:**\n",
        "- Normalise column names (e.g. `metric` → `metric_name`)\n",
        "- Parse strings to proper types (ints, floats, dates)\n",
        "- Handle missing values and duplicates\n",
        "- Derive report_date from filename or embedded metadata\n",
        "- Standardise platform names (e.g. \"IG\" → \"Instagram\")\n",
        "\n",
        "**Output:** A cleaned dataset ready for modelling (silver layer)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# STAGE 4: TRANSFORMATION — Clean and standardise raw data\n",
        "# =============================================================================\n",
        "# Read from raw storage, apply cleaning rules, produce silver-level data.\n",
        "# =============================================================================\n",
        "\n",
        "import json\n",
        "import re\n",
        "import pandas as pd\n",
        "from sqlalchemy import text\n",
        "\n",
        "\n",
        "def extract_report_date(source_file: str) -> str:\n",
        "    \"\"\"Derive report date from filename (e.g. 2024-01_report.pdf -> 2024-01-01).\"\"\"\n",
        "    match = re.search(r\"(\\d{4})[-_]?(\\d{2})\", source_file or \"\")\n",
        "    if match:\n",
        "        return f\"{match.group(1)}-{match.group(2)}-01\"\n",
        "    return None\n",
        "\n",
        "\n",
        "def transform_raw_to_silver(raw_df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    TRANSFORMATION: Clean and standardise raw social media metrics.\n",
        "    - Normalise column names\n",
        "    - Parse numeric values\n",
        "    - Standardise platform names\n",
        "    - Add report_date\n",
        "    \"\"\"\n",
        "    # Flatten raw_json if stored as JSON string\n",
        "    if \"raw_json\" in raw_df.columns:\n",
        "        expanded = raw_df[\"raw_json\"].apply(\n",
        "            lambda x: json.loads(x) if isinstance(x, str) else (x or {})\n",
        "        )\n",
        "        raw_df = pd.concat([raw_df.drop(columns=[\"raw_json\"]), pd.json_normalize(expanded)], axis=1)\n",
        "    \n",
        "    # Normalise column names for common variants\n",
        "    cols = {c.lower().replace(\" \", \"_\"): c for c in raw_df.columns}\n",
        "    raw_df = raw_df.rename(columns={v: k for k, v in cols.items()})\n",
        "    \n",
        "    # Map to canonical columns\n",
        "    platform_col = next((c for c in [\"platform\", \"channel\", \"social_platform\"] if c in raw_df.columns), None)\n",
        "    metric_col = next((c for c in [\"metric_name\", \"metric\", \"kpi\"] if c in raw_df.columns), None)\n",
        "    value_col = next((c for c in [\"value\", \"count\", \"metric_value\"] if c in raw_df.columns), None)\n",
        "    \n",
        "    df = pd.DataFrame()\n",
        "    df[\"platform\"] = raw_df[platform_col] if platform_col else \"unknown\"\n",
        "    df[\"metric_name\"] = raw_df[metric_col] if metric_col else \"unknown\"\n",
        "    df[\"value\"] = pd.to_numeric(raw_df[value_col], errors=\"coerce\").fillna(0)\n",
        "    df[\"report_date\"] = raw_df[\"source_file\"].apply(extract_report_date)\n",
        "    df[\"source_file\"] = raw_df[\"source_file\"]\n",
        "    \n",
        "    # Standardise platform names\n",
        "    platform_map = {\"ig\": \"Instagram\", \"fb\": \"Facebook\", \"li\": \"LinkedIn\", \"tw\": \"Twitter\", \"x\": \"X\"}\n",
        "    df[\"platform\"] = df[\"platform\"].replace(platform_map).str.strip()\n",
        "    \n",
        "    # Drop rows with invalid values\n",
        "    df = df[df[\"value\"] > 0].drop_duplicates()\n",
        "    return df\n",
        "\n",
        "\n",
        "# Read from raw storage and transform\n",
        "with engine.connect() as conn:\n",
        "    raw_df = pd.read_sql(text(\"SELECT * FROM raw_social_metrics\"), conn)\n",
        "\n",
        "silver_df = transform_raw_to_silver(raw_df)\n",
        "print(f\"Transformation complete: {len(silver_df)} cleaned records\")\n",
        "silver_df.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Stage 5: MODELLING — Analyst-Friendly Schema\n",
        "\n",
        "**What it is:** Design tables/views that make it **easy for analysts** to run common queries.\n",
        "\n",
        "**Modelling choices for social media metrics:**\n",
        "- **Fact table:** One row per (platform, metric, report_date) with value\n",
        "- **Dimension tables:** Platforms, metrics, dates for filtering\n",
        "- **Aggregations:** Pre-computed totals, growth rates, benchmarks\n",
        "\n",
        "**Goal:** Analysts can write simple SQL like `SELECT platform, SUM(value) FROM metrics GROUP BY platform` without wrestling with raw JSON or inconsistent column names."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66969f27",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# STAGE 5: MODELLING — Build analyst-friendly fact and dimension tables\n",
        "# =============================================================================\n",
        "# We persist the silver data into a star schema: fact_metrics + dim_platform, dim_metric\n",
        "# =============================================================================\n",
        "\n",
        "from sqlalchemy import text\n",
        "\n",
        "\n",
        "def create_modelled_schema(engine):\n",
        "    \"\"\"\n",
        "    MODELLING: Create fact and dimension tables for easy analyst querying.\n",
        "    - fact_social_metrics: (platform_id, metric_id, report_date, value)\n",
        "    - dim_platform: platform lookup\n",
        "    - dim_metric: metric lookup\n",
        "    \"\"\"\n",
        "    with engine.connect() as conn:\n",
        "        conn.execute(text(\"\"\"\n",
        "            CREATE TABLE IF NOT EXISTS dim_platform (\n",
        "                platform_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                platform_name VARCHAR(100) UNIQUE\n",
        "            )\n",
        "        \"\"\"))\n",
        "        conn.execute(text(\"\"\"\n",
        "            CREATE TABLE IF NOT EXISTS dim_metric (\n",
        "                metric_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                metric_name VARCHAR(100) UNIQUE\n",
        "            )\n",
        "        \"\"\"))\n",
        "        conn.execute(text(\"\"\"\n",
        "            CREATE TABLE IF NOT EXISTS fact_social_metrics (\n",
        "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                platform_id INTEGER,\n",
        "                metric_id INTEGER,\n",
        "                report_date DATE,\n",
        "                value REAL,\n",
        "                FOREIGN KEY (platform_id) REFERENCES dim_platform(platform_id),\n",
        "                FOREIGN KEY (metric_id) REFERENCES dim_metric(metric_id)\n",
        "            )\n",
        "        \"\"\"))\n",
        "        conn.commit()\n",
        "\n",
        "\n",
        "def load_modelled_data(engine, silver_df: pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Load transformed (silver) data into the modelled star schema.\n",
        "    \"\"\"\n",
        "    platforms = silver_df[\"platform\"].dropna().unique().tolist()\n",
        "    metrics = silver_df[\"metric_name\"].dropna().unique().tolist()\n",
        "    \n",
        "    with engine.connect() as conn:\n",
        "        for p in platforms:\n",
        "            conn.execute(text(\"INSERT OR IGNORE INTO dim_platform (platform_name) VALUES (:p)\"), {\"p\": p})\n",
        "        for m in metrics:\n",
        "            conn.execute(text(\"INSERT OR IGNORE INTO dim_metric (metric_name) VALUES (:m)\"), {\"m\": m})\n",
        "        conn.commit()\n",
        "        \n",
        "        platform_ids = pd.read_sql(text(\"SELECT platform_id, platform_name FROM dim_platform\"), conn)\n",
        "        metric_ids = pd.read_sql(text(\"SELECT metric_id, metric_name FROM dim_metric\"), conn)\n",
        "    \n",
        "    silver_df = silver_df.merge(platform_ids, left_on=\"platform\", right_on=\"platform_name\", how=\"left\")\n",
        "    silver_df = silver_df.merge(metric_ids, left_on=\"metric_name\", right_on=\"metric_name\", how=\"left\")\n",
        "    \n",
        "    with engine.connect() as conn:\n",
        "        for _, row in silver_df.iterrows():\n",
        "            conn.execute(text(\"\"\"\n",
        "                INSERT INTO fact_social_metrics (platform_id, metric_id, report_date, value)\n",
        "                VALUES (:pid, :mid, :rd, :v)\n",
        "            \"\"\"), {\"pid\": row[\"platform_id\"], \"mid\": row[\"metric_id\"], \"rd\": row[\"report_date\"] or \"2024-01-01\", \"v\": row[\"value\"]})\n",
        "        conn.commit()\n",
        "\n",
        "\n",
        "create_modelled_schema(engine)\n",
        "load_modelled_data(engine, silver_df)\n",
        "print(\"Modelling complete: fact_social_metrics + dim_platform + dim_metric populated\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Stage 6: SERVING — Query Data for Analysts\n",
        "\n",
        "**What it is:** The final layer where **end users (analysts)** consume data. They connect BI tools (Tableau, Power BI, Metabase) or run SQL directly against Postgres.\n",
        "\n",
        "**Serving from Postgres:**\n",
        "- The modelled tables (fact + dimensions) are the serving layer\n",
        "- Analysts write simple, readable SQL\n",
        "- Views can pre-join dimensions for convenience\n",
        "- Access control and query performance are managed at this layer\n",
        "\n",
        "**Example analyst queries:**\n",
        "- Total impressions by platform\n",
        "- Engagement trend over time\n",
        "- Top-performing platform by metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# STAGE 6: SERVING — Example analyst queries against the modelled data\n",
        "# =============================================================================\n",
        "# These are the kinds of queries analysts run from Postgres / BI tools.\n",
        "# =============================================================================\n",
        "\n",
        "# --- Query 1: Total value by platform (e.g. total impressions per platform) ---\n",
        "query_by_platform = \"\"\"\n",
        "SELECT p.platform_name, SUM(f.value) AS total_value\n",
        "FROM fact_social_metrics f\n",
        "JOIN dim_platform p ON f.platform_id = p.platform_id\n",
        "JOIN dim_metric m ON f.metric_id = m.metric_id\n",
        "GROUP BY p.platform_name\n",
        "ORDER BY total_value DESC\n",
        "\"\"\"\n",
        "\n",
        "# --- Query 2: Metrics by report date (trend over time) ---\n",
        "query_trend = \"\"\"\n",
        "SELECT f.report_date, p.platform_name, m.metric_name, SUM(f.value) AS total_value\n",
        "FROM fact_social_metrics f\n",
        "JOIN dim_platform p ON f.platform_id = p.platform_id\n",
        "JOIN dim_metric m ON f.metric_id = m.metric_id\n",
        "GROUP BY f.report_date, p.platform_name, m.metric_name\n",
        "ORDER BY f.report_date, p.platform_name\n",
        "\"\"\"\n",
        "\n",
        "# --- Query 3: Simple flat view for ad-hoc analysis ---\n",
        "query_flat = \"\"\"\n",
        "SELECT p.platform_name, m.metric_name, f.report_date, f.value\n",
        "FROM fact_social_metrics f\n",
        "JOIN dim_platform p ON f.platform_id = p.platform_id\n",
        "JOIN dim_metric m ON f.metric_id = m.metric_id\n",
        "ORDER BY f.report_date, p.platform_name, m.metric_name\n",
        "\"\"\"\n",
        "\n",
        "# Execute and display results\n",
        "from sqlalchemy import text\n",
        "\n",
        "with engine.connect() as conn:\n",
        "    df_platform = pd.read_sql(text(query_by_platform), conn)\n",
        "    print(\"\\n--- SERVING: Total by platform ---\")\n",
        "    print(df_platform.to_string(index=False))\n",
        "    \n",
        "    df_trend = pd.read_sql(text(query_trend), conn)\n",
        "    print(\"\\n--- SERVING: Trend by date/platform/metric ---\")\n",
        "    print(df_trend.to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Summary: End-to-End Flow\n",
        "\n",
        "| Stage | Artifact | Purpose |\n",
        "|-------|----------|---------|\n",
        "| **Source** | PDF files | Raw, unstructured input |\n",
        "| **Ingestion** | Extracted records | Structured extraction from PDFs |\n",
        "| **Raw Storage** | `raw_social_metrics` | Postgres bronze layer — immutable raw data |\n",
        "| **Transformation** | Silver DataFrame | Cleaned, validated, standardised |\n",
        "| **Modelling** | `fact_social_metrics`, `dim_*` | Star schema for analytics |\n",
        "| **Serving** | SQL queries / BI tools | Analysts query Postgres directly |\n",
        "\n",
        "**Production considerations:**\n",
        "- **Orchestration:** Use Airflow, Dagster, or Prefect to schedule ingestion and transformation\n",
        "- **Incremental loads:** Track `ingested_at` or `report_date` to avoid re-processing\n",
        "- **Data quality:** Add validation checks (e.g. Great Expectations) in transformation\n",
        "- **Serving:** Use views, materialised views, or a separate analytics schema for read-heavy workloads"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
